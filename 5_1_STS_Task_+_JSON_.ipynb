{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1hghJi7n6vv11Rn1igNxgrx5w_uldPyBL",
      "authorship_tag": "ABX9TyMDzzMLgDz34p9KKk4Rk2p0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Asato4931/Asato4931-CA_5_1-STS-Task-JSONL/blob/main/5_1_STS_Task_%2B_JSON_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "eZfKmdVl5Gr0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2da0c8c2-c52f-46e2-af55-d7e303c6c1ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sentence_transformers import models\n",
        " \n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from sklearn.feature_selection import r_regression\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "# Load pre-trained tokenizer\n",
        "transformer = models.Transformer(\"cl-tohoku/bert-base-japanese-v2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-v2\")\n",
        "#model = AutoModelForMaskedLM.from_pretrained(\"cl-tohoku/bert-base-japanese-v2\")\n",
        "pooling = models.Pooling(transformer.get_word_embedding_dimension(), pooling_mode_mean_tokens=True, pooling_mode_cls_token=False, pooling_mode_max_tokens=False)\n",
        "model = SentenceTransformer(modules=[transformer, pooling])\n",
        "\n",
        "\n",
        "valid_data = pd.read_json('/content/drive/MyDrive/Colab Notebooks/CA課題用/CA課題5/valid-v1.1.json', lines=True)\n",
        "\n",
        "\n",
        "valid_data = valid_data.loc[:,['sentence1','sentence2']]\n",
        "\n",
        "cos_sims = []\n",
        "pearson = []\n",
        "spearman = []\n",
        "\n",
        "\n",
        "\n",
        "#メモ cos類似度の入力は、横ベクトルじゃないと食べてくれない\n",
        "\n",
        "\n",
        "#Cos類似度\n",
        "\n",
        "for i in range(0,1457):\n",
        "  vector1_c = model.encode(valid_data.loc[i,'sentence1'])\n",
        "  vector2_c = model.encode(valid_data.loc[i,'sentence2'])\n",
        "\n",
        "\n",
        "  vector1_c = np.array(vector1_c).reshape(1,-1)\n",
        "  vector2_c = np.array(vector2_c).reshape(1,-1)\n",
        "\n",
        "\n",
        "  result = cosine_similarity(vector1_c,vector2_c)\n",
        "  cos_sims.append(result)\n",
        "\n",
        "\n",
        "#Pearson\n",
        "\n",
        "for i in range(0,1457):\n",
        "  vector1_p = model.encode(valid_data.loc[i,'sentence1'])\n",
        "  vector2_p = model.encode(valid_data.loc[i,'sentence2'])\n",
        "\n",
        "  result_p= stats.pearsonr(vector1_p,vector2_p)\n",
        "  pearson.append(result_p.statistic)\n",
        "\n",
        "\n",
        "#Spearman\n",
        "\n",
        "for i in range(0,1457):\n",
        "  vector1_s = model.encode(valid_data.loc[i,'sentence1'])\n",
        "  vector2_s = model.encode(valid_data.loc[i,'sentence2'])\n",
        "\n",
        "  vector1_s = np.array(vector1_s).reshape(1,-1)\n",
        "  vector2_s = np.array(vector2_s).reshape(1,-1)\n",
        "  result_s = stats.spearmanr(vector1_s,vector2_s ,axis=None)\n",
        "  spearman.append(result_s.statistic)\n",
        "\n",
        "pearson_df = pd.DataFrame(pearson)\n",
        "pearson_df[\"metrics\"] = \"pearson\"\n",
        "pearson_df = pearson_df.rename(columns= { pearson_df.columns[0]: \"score\"})\n",
        "pearson_df = pearson_df.reindex(columns=['metrics', 'score'])\n",
        "\n",
        "\n",
        "\n",
        "spearman_df = pd.DataFrame(spearman)\n",
        "spearman_df[\"metrics\"] = \"spearman\"\n",
        "spearman_df = spearman_df.rename(columns= { spearman_df.columns[0]: \"score\"})\n",
        "spearman_df = spearman_df.reindex(columns=['metrics', 'score'])\n",
        "\n",
        "\n",
        "frames = [pearson_df,spearman_df]\n",
        "\n",
        "result_5_1 = pd.concat(frames)\n",
        "\n",
        "result_5_1.to_json('/content/drive/MyDrive/Colab Notebooks/CA課題用/CA課題5/CA_5_1 STS Task Results.jsonl', force_ascii=False, lines = True, orient='records' )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers\n",
        "\n",
        "!apt-get install mecab mecab-ipadic-utf8 python-mecab libmecab-dev\n",
        "!pip install mecab-python3 \n",
        "\n",
        "!pip install fugashi\n",
        "\n",
        "!pip install unidic_lite"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v5wEfDNsA2v",
        "outputId": "e45cf44a-5b15-4284-9025-cd9a24dff90b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.9/dist-packages (2.2.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (0.1.98)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (4.65.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (0.13.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (4.28.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (2.0.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.22.4)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (0.15.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.25.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->sentence-transformers) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.12)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "E: Unable to locate package python-mecab\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mecab-python3 in /usr/local/lib/python3.9/dist-packages (1.0.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fugashi in /usr/local/lib/python3.9/dist-packages (1.2.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting unidic_lite\n",
            "  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: unidic_lite\n",
            "  Building wheel for unidic_lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic_lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658834 sha256=32d7f991c740f7705400e6ca24c800614104a246760ca6133dfe182bc621d393\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/9c/4f/2c115e896b4b6c584039ca19de3581d333856782ef108cdc5c\n",
            "Successfully built unidic_lite\n",
            "Installing collected packages: unidic_lite\n",
            "Successfully installed unidic_lite-1.0.8\n"
          ]
        }
      ]
    }
  ]
}